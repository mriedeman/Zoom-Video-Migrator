{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJtWVhFZjVPZVQwaWF0RHI1M3c3RzlBIiwiZXhwIjoxNjgyNjIxMTIyfQ.YpN8rvDA7Bd9Hr0OSlUaO2G7LF2mvYw65-OxMQvnV10'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import jwt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_jwt_token():\n",
    "# Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    # Get API credentials from environment variables\n",
    "    api_key = os.environ.get('ZOOM_API_KEY')\n",
    "    if not api_key:\n",
    "        raise ValueError('ZOOM_API_KEY environment variable not set')\n",
    "\n",
    "    api_secret = os.environ.get('ZOOM_API_SECRET')\n",
    "    if not api_secret:\n",
    "        raise ValueError('ZOOM_API_KEY environment variable not set')\n",
    "\n",
    "    payload = {\n",
    "        'iss': api_key,\n",
    "        'exp': datetime.utcnow() + timedelta(minutes=10)\n",
    "    }\n",
    "\n",
    "    token = jwt.encode(payload, api_secret)\n",
    "    return token\n",
    "\n",
    "generate_jwt_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ZOOM_API_KEY environment variable not set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m         json\u001b[39m.\u001b[39mdump(users_response, f)\n\u001b[0;32m     24\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUpdated the \u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m to include any new users on \u001b[39m\u001b[39m{\u001b[39;00mdatetime\u001b[39m.\u001b[39mnow()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m update_users()\n",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m, in \u001b[0;36mupdate_users\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m api_key \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mZOOM_API_KEY\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m api_key:\n\u001b[1;32m---> 10\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mZOOM_API_KEY environment variable not set\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m api_secret \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mZOOM_API_SECRET\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m api_secret:\n",
      "\u001b[1;31mValueError\u001b[0m: ZOOM_API_KEY environment variable not set"
     ]
    }
   ],
   "source": [
    "#Collect updated list of user's\n",
    "from zoomus import ZoomClient\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def update_users():\n",
    "    api_key = os.environ.get('ZOOM_API_KEY')\n",
    "    if not api_key:\n",
    "        raise ValueError('ZOOM_API_KEY environment variable not set')\n",
    "\n",
    "    api_secret = os.environ.get('ZOOM_API_SECRET')\n",
    "    if not api_secret:\n",
    "        raise ValueError('ZOOM_API_SECRET environment variable not set')\n",
    "\n",
    "    zoom = ZoomClient(api_key=api_key, api_secret=api_secret)\n",
    "\n",
    "    users_response = zoom.user.list().json()\n",
    "\n",
    "    filename = 'api_data/users/users.json'\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(users_response, f)\n",
    "\n",
    "    print(f\"Updated the {filename} to include any new users on {datetime.now()}\")\n",
    "\n",
    "update_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2021-01-01', '2021-01-31'),\n",
       " ('2021-02-01', '2021-02-28'),\n",
       " ('2021-03-01', '2021-03-31'),\n",
       " ('2021-04-01', '2021-04-30'),\n",
       " ('2021-05-01', '2021-05-31'),\n",
       " ('2021-06-01', '2021-06-30'),\n",
       " ('2021-07-01', '2021-07-31'),\n",
       " ('2021-08-01', '2021-08-31'),\n",
       " ('2021-09-01', '2021-09-30'),\n",
       " ('2021-10-01', '2021-10-31'),\n",
       " ('2021-11-01', '2021-11-30'),\n",
       " ('2021-12-01', '2021-12-31'),\n",
       " ('2022-01-01', '2022-01-31'),\n",
       " ('2022-02-01', '2022-02-28'),\n",
       " ('2022-03-01', '2022-03-31'),\n",
       " ('2022-04-01', '2022-04-30'),\n",
       " ('2022-05-01', '2022-05-31'),\n",
       " ('2022-06-01', '2022-06-30'),\n",
       " ('2022-07-01', '2022-07-31'),\n",
       " ('2022-08-01', '2022-08-31'),\n",
       " ('2022-09-01', '2022-09-30'),\n",
       " ('2022-10-01', '2022-10-31'),\n",
       " ('2022-11-01', '2022-11-30'),\n",
       " ('2022-12-01', '2022-12-31'),\n",
       " ('2023-01-01', '2023-01-31'),\n",
       " ('2023-02-01', '2023-02-28'),\n",
       " ('2023-03-01', '2023-03-31'),\n",
       " ('2023-04-01', '2023-04-30')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def date_params():\n",
    "    start_date_str = \"2021-01-01\" #first zoom recording was 2021-01-03\n",
    "    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "    now = datetime.now()\n",
    "\n",
    "    date_ranges = []\n",
    "    while start_date < now:\n",
    "        # Get the first day of the current month\n",
    "        start_of_month = datetime(start_date.year, start_date.month, 1)\n",
    "\n",
    "        #Get the last day of the current month\n",
    "        end_of_month = (start_of_month + timedelta(days=32)).replace(day=1) - timedelta(days=1)\n",
    "\n",
    "        date_ranges.append((start_of_month.strftime('%Y-%m-%d'), end_of_month.strftime('%Y-%m-%d')))\n",
    "\n",
    "        start_date = end_of_month + timedelta(days=1)\n",
    "\n",
    "    #print(\"Created a tupled list of FROM and TO parameters for the Zoom API Query\")\n",
    "    return date_ranges\n",
    "\n",
    "date_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting recording results for user Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\n",
      "Collecting recording results for user Jim Smith 2UsBai07StauaBqBbbjxeA\n",
      "Collecting recording results for user Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\n",
      "Collecting recording results for user STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\n",
      "Collecting recording results for user Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\n",
      "Collecting recording results for user KP Auditorium f98iwlEeRpS2XAipG1j2YA\n",
      "Collecting recording results for user Dipayan Roy k3gb5SCTRDO00ko71PWNqw\n",
      "Collecting recording results for user Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\n",
      "Collecting recording results for user KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\n",
      "Collecting recording results for user Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\n",
      "DATA COLLECTION COMPLETE AT 2023-04-27 11:37:51.155837\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import jwt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def collect_legacy_data():\n",
    "    #get user ids for API call \"/users/{user_id}/recordings\"\n",
    "    with open('./api_data/users/users.json') as f:\n",
    "        users = json.load(f)\n",
    "\n",
    "\n",
    "    #Collect all recording data, one user at a time\n",
    "    for user in users['users']:\n",
    "\n",
    "        user_first_name = user['first_name']\n",
    "        user_last_name = user['last_name']\n",
    "        user_id = user['id']\n",
    "        print(f'Collecting recording results for user {user_first_name} {user_last_name} {user_id}')\n",
    "        \n",
    "        for start, end in date_params():\n",
    "\n",
    "            #Avoid sending API query with end date greater than today's date\n",
    "            now = datetime.now()\n",
    "            if datetime.strptime(end, '%Y-%m-%d') > now:\n",
    "                break\n",
    "            \n",
    "            # API access\n",
    "            base_url = \"https://api.zoom.us/v2\"\n",
    "            token = generate_jwt_token()\n",
    "            headers = {'Authorization': f'Bearer {token}'}\n",
    "            url = base_url + f\"/users/{user_id}/recordings\"\n",
    "            params = { \"from\": start,\n",
    "                    \"to\": end}\n",
    "            \n",
    "            response = requests.get(url, headers=headers, params = params)\n",
    "            recordings_data = response.json()\n",
    "            \n",
    "            # Keep making API requests until all pages are collected\n",
    "            next_token = recordings_data.get('next_page_token')\n",
    "            while next_token:\n",
    "                \n",
    "                url = base_url + f\"/users/{user_id}/recordings\"\n",
    "                params = { \"from\": start, \"to\": end, \"next_page_token\": next_token}\n",
    "                response = requests.get(url, headers=headers, params=params)\n",
    "                next_page_data = response.json()\n",
    "                recordings_data['meetings'] += next_page_data['meetings']\n",
    "                next_token = next_page_data.get('next_page_token')\n",
    "                if next_token == recordings_data.get('next_page_token'):\n",
    "                    break\n",
    "                recordings_data['next_page_token'] = next_token\n",
    "                \n",
    "            #print(f'Collecting recording results for user {user_first_name} {user_last_name} from {start} to {end}')\n",
    "\n",
    "            #Store data\n",
    "            directory = f'api_data/recordings/{user_first_name} {user_last_name} {user_id}'\n",
    "            filename = f'api_data/recordings/{user_first_name} {user_last_name} {user_id}/{start} to {end}.json'\n",
    "\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            \n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(recordings_data, f)\n",
    "\n",
    "    print(f\"DATA COLLECTION COMPLETE AT {datetime.now()}\")\n",
    "\n",
    "collect_legacy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2023-04-17', '2023-04-23')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_previous_week_dates():\n",
    "    \"\"\"Collects the start and end dates for the previous week\"\"\"\n",
    "    today = datetime.now().date() - timedelta(days=7)\n",
    "    start_date = today - timedelta(days=today.weekday())\n",
    "    end_date = start_date + timedelta(days=6)\n",
    "    return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "get_previous_week_dates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting recording results for user Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\n",
      "Collecting recording results for user Jim Smith 2UsBai07StauaBqBbbjxeA\n",
      "Collecting recording results for user Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\n",
      "Collecting recording results for user STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\n",
      "Collecting recording results for user Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\n",
      "Collecting recording results for user KP Auditorium f98iwlEeRpS2XAipG1j2YA\n",
      "Collecting recording results for user Dipayan Roy k3gb5SCTRDO00ko71PWNqw\n",
      "Collecting recording results for user Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\n",
      "Collecting recording results for user KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\n",
      "Collecting recording results for user Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\n",
      "DATA COLLECTION COMPLETE AT 2023-04-27 14:22:02.983948\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import jwt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def collect_weekly_data(start_date=None, end_date=None):\n",
    "    \"\"\"Collects weekly recording data. If no date is specified, it collects the most recent weeks data. To specify a data enter it as a string in \"YYYY-MM-DD\" format\"\"\"\n",
    "    if start_date is None or end_date is None:\n",
    "        # Get the current week's start and end dates\n",
    "        start_date, end_date = get_previous_week_dates()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #get user ids for API call \"/users/{user_id}/recordings\"\n",
    "    with open('./api_data/users/users.json') as f:\n",
    "        users = json.load(f)\n",
    "\n",
    "\n",
    "    #Collect all recording data, one user at a time\n",
    "    for user in users['users']:\n",
    "\n",
    "        user_first_name = user['first_name']\n",
    "        user_last_name = user['last_name']\n",
    "        user_id = user['id']\n",
    "        print(f'Collecting recording results for user {user_first_name} {user_last_name} {user_id}')\n",
    "\n",
    "\n",
    "        #Avoid sending API query with end date greater than today's date\n",
    "        now = datetime.now()\n",
    "        if datetime.strptime(end_date, '%Y-%m-%d') > now:\n",
    "            break\n",
    "\n",
    "        # API access\n",
    "        base_url = \"https://api.zoom.us/v2\"\n",
    "        token = generate_jwt_token()\n",
    "        headers = {'Authorization': f'Bearer {token}'}\n",
    "        url = base_url + f\"/users/{user_id}/recordings\"\n",
    "        params = { \"from\": start_date,\n",
    "                \"to\": end_date}\n",
    "\n",
    "        response = requests.get(url, headers=headers, params = params)\n",
    "        recordings_data = response.json()\n",
    "\n",
    "        # Keep making API requests until all pages are collected\n",
    "        next_token = recordings_data.get('next_page_token')\n",
    "        while next_token:\n",
    "\n",
    "            url = base_url + f\"/users/{user_id}/recordings\"\n",
    "            params = { \"from\": start_date, \"to\": end_date, \"next_page_token\": next_token}\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "            next_page_data = response.json()\n",
    "            recordings_data['meetings'] += next_page_data['meetings']\n",
    "            next_token = next_page_data.get('next_page_token')\n",
    "            if next_token == recordings_data.get('next_page_token'):\n",
    "                break\n",
    "            recordings_data['next_page_token'] = next_token\n",
    "\n",
    "        #print(f'Collecting recording results for user {user_first_name} {user_last_name} from {start} to {end}')\n",
    "\n",
    "        #Store data\n",
    "        directory = f'api_data/weekly_data/{user_first_name} {user_last_name} {user_id}'\n",
    "        filename = f'api_data/weekly_data/{user_first_name} {user_last_name} {user_id}/{start_date} to {end_date}.json'\n",
    "\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(recordings_data, f)\n",
    "\n",
    "    print(f\"DATA COLLECTION COMPLETE AT {datetime.now()}\")\n",
    "\n",
    "collect_weekly_data(\"2022-4-1\",\"2022-4-8\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original buffer download method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaIoBaseUpload\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def download_video_to_buffer(download_url):\n",
    "    token = generate_jwt_token()\n",
    "    response = requests.get(download_url + \"?access_token=\" + token, stream=True)\n",
    "    video_bytes = io.BytesIO()\n",
    "    for chunk in response.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            video_bytes.write(chunk)\n",
    "    video_bytes.seek(0)\n",
    "    return video_bytes\n",
    "\n",
    "\n",
    "def upload_buffer_to_drive(video_bytes, filename):\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    # Google Drive API credentials\n",
    "    if 'GOOGLE_AUTH_FILE' not in os.environ:\n",
    "        raise ValueError('GOOGLE_AUTH_FILE environment variable not set')\n",
    "    google_api_credentials = os.environ.get('GOOGLE_AUTH_FILE')\n",
    "    creds = service_account.Credentials.from_service_account_file(google_api_credentials)\n",
    "\n",
    "    if 'GOOGLE_PARENT_FOLDER_ID' not in os.environ:\n",
    "        raise ValueError('GOOGLE_PARENT_FOLDER_ID environment variable not set')\n",
    "    parent_folder_id = os.environ.get('GOOGLE_PARENT_FOLDER_ID')\n",
    "\n",
    "    drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    # Create a MediaIoBaseUpload object using the video_bytes\n",
    "    media = MediaIoBaseUpload(video_bytes, mimetype='video/mp4', chunksize=1024 * 1024, resumable=True)\n",
    "\n",
    "    # Upload video file to Google Drive\n",
    "    file_metadata = {'name': filename, 'parents': [parent_folder_id]}\n",
    "    file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "\n",
    "    print(f\"{filename} uploaded successfully to Google Drive with a File ID of: {file.get('id')}.\")\n",
    "\n",
    "\n",
    "def migrate_videos(json_file, first_name, last_name, user_id):\n",
    "    \"\"\"Migrates All The Videos in a Single JSON File to Google Drive\"\"\"\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for meeting in data['meetings']:\n",
    "        for recording in meeting['recording_files']:\n",
    "            if recording['file_type'] == \"MP4\":\n",
    "\n",
    "                download_url = recording['download_url']\n",
    "                date_string = recording['recording_start']\n",
    "                dt_object = datetime.strptime(date_string, '%Y-%m-%dT%H:%M:%SZ')\n",
    "                timestamp_string = dt_object.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "                filename = f\"{first_name} {last_name} {user_id} {timestamp_string}\"\n",
    "                video = download_video_to_buffer(download_url)\n",
    "                #upload_buffer_to_drive(video, filename)\n",
    "\n",
    "                with download_video_to_buffer(download_url) as video:\n",
    "                    upload_buffer_to_drive(video, filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download To Buffer, Then Upload to Google Drive: 1.5 MB/s data transfer rate (~416 hours to transfer 3TB of data) with my current internet connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2021-01-01 to 2021-01-31.json\n",
      "./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2021-01-01 to 2021-01-31.json\n",
      "./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2021-01-01 to 2021-01-31.json\n",
      "./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2021-01-01 to 2021-01-31.json\n",
      "./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2021-01-01 to 2021-01-31.json\n",
      "./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2021-01-01 to 2021-01-31.json\n",
      "./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2021-01-01 to 2021-01-31.json\n",
      "./api_data/recordings\\KP Auditorium f98iwlEeRpS2XAipG1j2YA\\2021-01-01 to 2021-01-31.json\n",
      "./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2021-01-01 to 2021-01-31.json\n",
      "./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2021-01-01 to 2021-01-31.json\n",
      "FINISHED BATCH JOB FOR '2021-01-01 to 2021-01-31.json' FILES\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import jwt\n",
    "from datetime import datetime, timedelta\n",
    "from zoomus import ZoomClient\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaIoBaseUpload\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def upload_video_to_google_drive(download_url, filename):\n",
    "    \"\"\"Uploads a single video to Google Drive given the download_url and filename\"\"\"\n",
    "\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    # Google Drive API credentials\n",
    "    if 'GOOGLE_AUTH_FILE' not in os.environ:\n",
    "        raise ValueError('GOOGLE_AUTH_FILE environment variable not set')\n",
    "    google_api_credentials = os.environ.get('GOOGLE_AUTH_FILE')\n",
    "    creds = service_account.Credentials.from_service_account_file(google_api_credentials)\n",
    "\n",
    "    if 'GOOGLE_PARENT_FOLDER_ID' not in os.environ:\n",
    "        raise ValueError('GOOGLE_PARENT_FOLDER_ID environment variable not set')\n",
    "    parent_folder_id = os.environ.get('GOOGLE_PARENT_FOLDER_ID')\n",
    "\n",
    "    drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    # Upload video file to Google Drive\n",
    "    file_metadata = {'name': filename, 'parents': [parent_folder_id]}\n",
    "    file = None\n",
    "    try:\n",
    "        token = generate_jwt_token()\n",
    "        response = requests.get(download_url + \"?access_token=\" + token, stream=True)\n",
    "        with io.BytesIO() as video_bytes:\n",
    "            # Download video data in chunks and write to BytesIO object\n",
    "            for chunk in response.iter_content(chunk_size=1024 * 1024):\n",
    "                if chunk:\n",
    "                    video_bytes.write(chunk)\n",
    "            video_bytes.seek(0)\n",
    "\n",
    "            # Create a MediaIoBaseUpload object using the video_bytes\n",
    "            media = MediaIoBaseUpload(video_bytes, mimetype='video/mp4', chunksize=1024 * 1024, resumable=True)\n",
    "\n",
    "            # Upload video file to Google Drive\n",
    "            file_metadata = {'name': filename, 'parents': [parent_folder_id]}\n",
    "            file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "            \n",
    "    except HttpError as error:\n",
    "        print(f'An error occurred: {error}')\n",
    "    \n",
    "    print(f\"{filename} uploaded successfully to Google Drive with a File ID of: {file.get('id')}.\")\n",
    "\n",
    "def migrate_videos_to_google_drive(json_file, first_name, last_name, user_id):\n",
    "    \"\"\"Migrates All The Videos in a Single JSON File to Google Drive\"\"\"\n",
    "\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for meeting in data['meetings']:\n",
    "        for recording in meeting['recording_files']:\n",
    "            if recording['file_type'] == \"MP4\":\n",
    "\n",
    "                download_url = recording['download_url']\n",
    "                date_string = recording['recording_start']\n",
    "                dt_object = datetime.strptime(date_string, '%Y-%m-%dT%H:%M:%SZ')\n",
    "                timestamp_string = dt_object.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "                filename = f\"{first_name} {last_name} {user_id} {timestamp_string}\"\n",
    "\n",
    "                upload_video_to_google_drive(download_url, filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batch_job(start_date=None, end_date=None):\n",
    "    \"\"\"Selectively Iterates Through The Recording Directory to Batch The Data Transfer\"\"\"\n",
    "    \n",
    "    if start_date is None or end_date is None:\n",
    "        # Get the current week's start and end dates\n",
    "        start_date, end_date = get_previous_week_dates()\n",
    "    \n",
    "    recording_directory = \"./api_data/recordings\"\n",
    "    for user_folder in os.listdir(recording_directory):\n",
    "        path = os.path.isdir(os.path.join(recording_directory, user_folder))\n",
    "        if path:\n",
    "                for file_name in os.listdir(os.path.join(recording_directory, user_folder)):\n",
    "                    if file_name.startswith(start_date) and file_name.endswith(end_date + \".json\"):\n",
    "                            \n",
    "                            file_path = os.path.join(recording_directory, user_folder, file_name)\n",
    "                            first_name = user_folder.split(\" \")[0]\n",
    "                            last_name = user_folder.split(\" \")[1]\n",
    "                            user_id = user_folder.split(\" \")[2]\n",
    "\n",
    "                            print(file_path)\n",
    "                            migrate_videos_to_google_drive(file_path, first_name, last_name, user_id)\n",
    "\n",
    "    print(F\"FINISHED BATCH JOB FOR '{start_date} to {end_date}.json' FILES\")\n",
    "\n",
    "batch_job(date_params()[3][0] , date_params()[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./api_data/recordings\\KP Auditorium f98iwlEeRpS2XAipG1j2YA\\2023-04-24 to 2023-04-30.json\n",
      "FINISHED BATCH JOB FOR '2023-04-24 to 2023-04-30.json' FILES\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def batch_job(start_date=None, end_date=None):\n",
    "    \"\"\"Selectively Iterates Through The Recording Directory to Batch The Data Transfer\"\"\"\n",
    "    \n",
    "    if start_date is None or end_date is None:\n",
    "        # Get the current week's start and end dates\n",
    "        start_date, end_date = get_previous_week_dates()\n",
    "        \n",
    "    recording_directory = \"./api_data/recordings\"\n",
    "    for user_folder in os.listdir(recording_directory):\n",
    "        path = os.path.isdir(os.path.join(recording_directory, user_folder))\n",
    "        if path:\n",
    "                for file_name in os.listdir(os.path.join(recording_directory, user_folder)):\n",
    "                    if file_name.startswith(start_date) and file_name.endswith(end_date + \".json\"):\n",
    "                            \n",
    "                            file_path = os.path.join(recording_directory, user_folder, file_name)\n",
    "                            first_name = user_folder.split(\" \")[0]\n",
    "                            last_name = user_folder.split(\" \")[1]\n",
    "                            user_id = user_folder.split(\" \")[2]\n",
    "\n",
    "                            print(file_path)\n",
    "                            #migrate_videos_to_google_drive(file_path, first_name, last_name, user_id)\n",
    "    print(F\"FINISHED BATCH JOB FOR '{start_date} to {end_date}.json' FILES\")\n",
    "#batch_job(date_params()[26][0] , date_params()[26][1])\n",
    "\n",
    "#get_current_week_dates()\n",
    "batch_job()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload Each Data Chunk as it is Uploaded Resulted in 1.5 MB/s data transfer rate (~416 hours to transfer 3TB of data) with my current internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dipayan Roy k3gb5SCTRDO00ko71PWNqw 2021-01-29_15-12-58 uploaded successfully to Google Drive with a File ID of: 1xqRYSAu80iou-jGAk3vZBIpiY5_j_BGM.\n",
      "Dipayan Roy k3gb5SCTRDO00ko71PWNqw 2021-01-28_15-56-46 uploaded successfully to Google Drive with a File ID of: 1VLV9CVC6Z3z3W2EFJLpjFY1PrbI5bJXe.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 113\u001b[0m\n\u001b[0;32m    110\u001b[0m last_name \u001b[39m=\u001b[39m user_folder\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m    111\u001b[0m user_id \u001b[39m=\u001b[39m user_folder\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m2\u001b[39m]\n\u001b[1;32m--> 113\u001b[0m migrate_videos_to_google_drive(file_path, first_name, last_name, user_id)\n",
      "Cell \u001b[1;32mIn[8], line 98\u001b[0m, in \u001b[0;36mmigrate_videos_to_google_drive\u001b[1;34m(json_file, first_name, last_name, user_id)\u001b[0m\n\u001b[0;32m     95\u001b[0m timestamp_string \u001b[39m=\u001b[39m dt_object\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     96\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfirst_name\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mlast_name\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00muser_id\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mtimestamp_string\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 98\u001b[0m upload_video_to_google_drive(download_url, filename)\n",
      "Cell \u001b[1;32mIn[8], line 77\u001b[0m, in \u001b[0;36mupload_video_to_google_drive\u001b[1;34m(download_url, filename)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[39m# Upload video file to Google Drive\u001b[39;00m\n\u001b[0;32m     76\u001b[0m         file_metadata \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: filename, \u001b[39m'\u001b[39m\u001b[39mparents\u001b[39m\u001b[39m'\u001b[39m: [parent_folder_id]}\n\u001b[1;32m---> 77\u001b[0m         file \u001b[39m=\u001b[39m drive_service\u001b[39m.\u001b[39;49mfiles()\u001b[39m.\u001b[39;49mcreate(body\u001b[39m=\u001b[39;49mfile_metadata, media_body\u001b[39m=\u001b[39;49mmedia, fields\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mexecute()\n\u001b[0;32m     78\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m uploaded successfully to Google Drive with a File ID of: \u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[39mexcept\u001b[39;00m HttpError \u001b[39mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\Michael\\anaconda3\\envs\\zoom\\Lib\\site-packages\\googleapiclient\\_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[39melif\u001b[39;00m positional_parameters_enforcement \u001b[39m==\u001b[39m POSITIONAL_WARNING:\n\u001b[0;32m    129\u001b[0m         logger\u001b[39m.\u001b[39mwarning(message)\n\u001b[1;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Michael\\anaconda3\\envs\\zoom\\Lib\\site-packages\\googleapiclient\\http.py:902\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    900\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[39mwhile\u001b[39;00m body \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m         _, body \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_chunk(http\u001b[39m=\u001b[39;49mhttp, num_retries\u001b[39m=\u001b[39;49mnum_retries)\n\u001b[0;32m    903\u001b[0m     \u001b[39mreturn\u001b[39;00m body\n\u001b[0;32m    905\u001b[0m \u001b[39m# Non-resumable case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Michael\\anaconda3\\envs\\zoom\\Lib\\site-packages\\googleapiclient\\_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[39melif\u001b[39;00m positional_parameters_enforcement \u001b[39m==\u001b[39m POSITIONAL_WARNING:\n\u001b[0;32m    129\u001b[0m         logger\u001b[39m.\u001b[39mwarning(message)\n\u001b[1;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Michael\\anaconda3\\envs\\zoom\\Lib\\site-packages\\googleapiclient\\http.py:1084\u001b[0m, in \u001b[0;36mHttpRequest.next_chunk\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m   1078\u001b[0m     LOGGER\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m   1079\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRetry #\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m for media upload: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, following status: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1080\u001b[0m         \u001b[39m%\u001b[39m (retry_num, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muri, resp\u001b[39m.\u001b[39mstatus)\n\u001b[0;32m   1081\u001b[0m     )\n\u001b[0;32m   1083\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1084\u001b[0m     resp, content \u001b[39m=\u001b[39m http\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m   1085\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresumable_uri, method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPUT\u001b[39;49m\u001b[39m\"\u001b[39;49m, body\u001b[39m=\u001b[39;49mdata, headers\u001b[39m=\u001b[39;49mheaders\n\u001b[0;32m   1086\u001b[0m     )\n\u001b[0;32m   1087\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m   1088\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_error_state \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Michael\\anaconda3\\envs\\zoom\\Lib\\site-packages\\google_auth_httplib2.py:218\u001b[0m, in \u001b[0;36mAuthorizedHttp.request\u001b[1;34m(self, uri, method, body, headers, redirections, connection_type, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m     body_stream_position \u001b[39m=\u001b[39m body\u001b[39m.\u001b[39mtell()\n\u001b[0;32m    217\u001b[0m \u001b[39m# Make the request.\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m response, content \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhttp\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    219\u001b[0m     uri,\n\u001b[0;32m    220\u001b[0m     method,\n\u001b[0;32m    221\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    222\u001b[0m     headers\u001b[39m=\u001b[39;49mrequest_headers,\n\u001b[0;32m    223\u001b[0m     redirections\u001b[39m=\u001b[39;49mredirections,\n\u001b[0;32m    224\u001b[0m     connection_type\u001b[39m=\u001b[39;49mconnection_type,\n\u001b[0;32m    225\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    226\u001b[0m )\n\u001b[0;32m    228\u001b[0m \u001b[39m# If the response indicated that the credentials needed to be\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[39m# refreshed, then refresh the credentials and re-attempt the\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[39m# request.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[39m# A stored token may expire between the time it is retrieved and\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[39m# the time the request is made, so we may need to try twice.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    234\u001b[0m     response\u001b[39m.\u001b[39mstatus \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_refresh_status_codes\n\u001b[0;32m    235\u001b[0m     \u001b[39mand\u001b[39;00m _credential_refresh_attempt \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_refresh_attempts\n\u001b[0;32m    236\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Michael\\anaconda3\\envs\\zoom\\Lib\\site-packages\\httplib2\\__init__.py:1724\u001b[0m, in \u001b[0;36mHttp.request\u001b[1;34m(self, uri, method, body, headers, redirections, connection_type)\u001b[0m\n\u001b[0;32m   1722\u001b[0m             content \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1723\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1724\u001b[0m             (response, content) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m   1725\u001b[0m                 conn, authority, uri, request_uri, method, body, headers, redirections, cachekey,\n\u001b[0;32m   1726\u001b[0m             )\n\u001b[0;32m   1727\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1728\u001b[0m     is_timeout \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(e, socket\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\Michael\\anaconda3\\envs\\zoom\\Lib\\site-packages\\httplib2\\__init__.py:1444\u001b[0m, in \u001b[0;36mHttp._request\u001b[1;34m(self, conn, host, absolute_uri, request_uri, method, body, headers, redirections, cachekey)\u001b[0m\n\u001b[0;32m   1441\u001b[0m \u001b[39mif\u001b[39;00m auth:\n\u001b[0;32m   1442\u001b[0m     auth\u001b[39m.\u001b[39mrequest(method, request_uri, headers, body)\n\u001b[1;32m-> 1444\u001b[0m (response, content) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn_request(conn, request_uri, method, body, headers)\n\u001b[0;32m   1446\u001b[0m \u001b[39mif\u001b[39;00m auth:\n\u001b[0;32m   1447\u001b[0m     \u001b[39mif\u001b[39;00m auth\u001b[39m.\u001b[39mresponse(response, body):\n",
      "File \u001b[1;32mc:\\Users\\Michael\\anaconda3\\envs\\zoom\\Lib\\site-packages\\httplib2\\__init__.py:1367\u001b[0m, in \u001b[0;36mHttp._conn_request\u001b[1;34m(self, conn, request_uri, method, body, headers)\u001b[0m\n\u001b[0;32m   1365\u001b[0m     \u001b[39mif\u001b[39;00m conn\u001b[39m.\u001b[39msock \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1366\u001b[0m         conn\u001b[39m.\u001b[39mconnect()\n\u001b[1;32m-> 1367\u001b[0m     conn\u001b[39m.\u001b[39;49mrequest(method, request_uri, body, headers)\n\u001b[0;32m   1368\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mtimeout:\n\u001b[0;32m   1369\u001b[0m     conn\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Michael\\anaconda3\\envs\\zoom\\Lib\\http\\client.py:1282\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1279\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\u001b[39mself\u001b[39m, method, url, body\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, headers\u001b[39m=\u001b[39m{}, \u001b[39m*\u001b[39m,\n\u001b[0;32m   1280\u001b[0m             encode_chunked\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   1281\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1282\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\Michael\\anaconda3\\envs\\zoom\\Lib\\http\\client.py:1328\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1324\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(body, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1325\u001b[0m     \u001b[39m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m     \u001b[39m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 1328\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\Michael\\anaconda3\\envs\\zoom\\Lib\\http\\client.py:1277\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1277\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\Michael\\anaconda3\\envs\\zoom\\Lib\\http\\client.py:1076\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1072\u001b[0m     \u001b[39mif\u001b[39;00m encode_chunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_http_vsn \u001b[39m==\u001b[39m \u001b[39m11\u001b[39m:\n\u001b[0;32m   1073\u001b[0m         \u001b[39m# chunked encoding\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m         chunk \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(chunk)\u001b[39m:\u001b[39;00m\u001b[39mX\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m chunk \\\n\u001b[0;32m   1075\u001b[0m             \u001b[39m+\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m-> 1076\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(chunk)\n\u001b[0;32m   1078\u001b[0m \u001b[39mif\u001b[39;00m encode_chunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_http_vsn \u001b[39m==\u001b[39m \u001b[39m11\u001b[39m:\n\u001b[0;32m   1079\u001b[0m     \u001b[39m# end chunked transfer\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Michael\\anaconda3\\envs\\zoom\\Lib\\http\\client.py:998\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    996\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m\"\u001b[39m\u001b[39mhttp.client.send\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m, data)\n\u001b[0;32m    997\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 998\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msock\u001b[39m.\u001b[39;49msendall(data)\n\u001b[0;32m    999\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   1000\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mIterable):\n",
      "File \u001b[1;32mc:\\Users\\Michael\\anaconda3\\envs\\zoom\\Lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.sendall\u001b[1;34m(self, data, flags)\u001b[0m\n\u001b[0;32m   1239\u001b[0m         amount \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(byte_view)\n\u001b[0;32m   1240\u001b[0m         \u001b[39mwhile\u001b[39;00m count \u001b[39m<\u001b[39m amount:\n\u001b[1;32m-> 1241\u001b[0m             v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(byte_view[count:])\n\u001b[0;32m   1242\u001b[0m             count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m v\n\u001b[0;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Michael\\anaconda3\\envs\\zoom\\Lib\\ssl.py:1210\u001b[0m, in \u001b[0;36mSSLSocket.send\u001b[1;34m(self, data, flags)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1207\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1208\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to send() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1209\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1210\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mwrite(data)\n\u001b[0;32m   1211\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1212\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msend(data, flags)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import jwt\n",
    "from datetime import datetime, timedelta\n",
    "from zoomus import ZoomClient\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaIoBaseUpload\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "class DownloadUploadStream(io.RawIOBase):\n",
    "    def __init__(self, download_url, token):\n",
    "        super().__init__()\n",
    "        self.download_url = download_url\n",
    "        self.token = token\n",
    "        self.response = requests.get(self.download_url + \"?access_token=\" + self.token, stream=True)\n",
    "        self.iterator = self.response.iter_content(chunk_size=1024 * 1024)\n",
    "        self.position = 0\n",
    "\n",
    "    def read(self, size=-1):\n",
    "        try:\n",
    "            chunk = next(self.iterator)\n",
    "            self.position += len(chunk)\n",
    "            return chunk\n",
    "        except StopIteration:\n",
    "            return b''\n",
    "\n",
    "    def seekable(self):\n",
    "        return True\n",
    "\n",
    "    def seek(self, offset, whence=0):\n",
    "        if whence == 0:\n",
    "            self.position = offset\n",
    "        elif whence == 1:\n",
    "            self.position += offset\n",
    "        elif whence == 2:\n",
    "            # Not supported\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Invalid whence value\")\n",
    "        # Reset the iterator to the correct position in the stream\n",
    "        self.response = requests.get(self.download_url + \"?access_token=\" + self.token, \n",
    "                                      stream=True, headers={'Range': f'bytes={self.position}-'})\n",
    "        self.iterator = self.response.iter_content(chunk_size=1024 * 1024)\n",
    "    \n",
    "\n",
    "def upload_video_to_google_drive(download_url, filename):\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    # Google Drive API credentials\n",
    "    if 'GOOGLE_AUTH_FILE' not in os.environ:\n",
    "        raise ValueError('GOOGLE_AUTH_FILE environment variable not set')\n",
    "    google_api_credentials = os.environ.get('GOOGLE_AUTH_FILE')\n",
    "    creds = service_account.Credentials.from_service_account_file(google_api_credentials)\n",
    "\n",
    "    if 'GOOGLE_PARENT_FOLDER_ID' not in os.environ:\n",
    "        raise ValueError('GOOGLE_PARENT_FOLDER_ID environment variable not set')\n",
    "    parent_folder_id = os.environ.get('GOOGLE_PARENT_FOLDER_ID')\n",
    "\n",
    "    drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    try:\n",
    "        token = generate_jwt_token()\n",
    "        with DownloadUploadStream(download_url, token) as stream, io.BytesIO() as buffer:\n",
    "            for chunk in stream:\n",
    "                buffer.write(chunk)\n",
    "            media = MediaIoBaseUpload(buffer, mimetype='video/mp4', chunksize=1024 * 1024, resumable=True)\n",
    "\n",
    "            # Upload video file to Google Drive\n",
    "            file_metadata = {'name': filename, 'parents': [parent_folder_id]}\n",
    "            file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "            print(f\"{filename} uploaded successfully to Google Drive with a File ID of: {file.get('id')}.\")\n",
    "    except HttpError as error:\n",
    "        print(f'An error occurred: {error}')\n",
    "\n",
    "\n",
    "def migrate_videos_to_google_drive(json_file, first_name, last_name, user_id):\n",
    "    \"\"\"Migrates All The Videos in a Single JSON File to Google Drive\"\"\"\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for meeting in data['meetings']:\n",
    "        for recording in meeting['recording_files']:\n",
    "            if recording['file_type'] == \"MP4\":\n",
    "\n",
    "                download_url = recording['download_url']\n",
    "                date_string = recording['recording_start']\n",
    "                dt_object = datetime.strptime(date_string, '%Y-%m-%dT%H:%M:%SZ')\n",
    "                timestamp_string = dt_object.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "                filename = f\"{first_name} {last_name} {user_id} {timestamp_string}\"\n",
    "\n",
    "                upload_video_to_google_drive(download_url, filename)\n",
    "\n",
    "\n",
    "recording_directory = \"./api_data/recordings\"\n",
    "for user_folder in os.listdir(recording_directory):\n",
    "    path = os.path.isdir(os.path.join(recording_directory, user_folder))\n",
    "    if path:\n",
    "            for file_name in os.listdir(os.path.join(recording_directory, user_folder)):\n",
    "                if file_name.endswith(\".json\"):\n",
    "                        \n",
    "                        file_path = os.path.join(recording_directory, user_folder, file_name)\n",
    "                        first_name = user_folder.split(\" \")[0]\n",
    "                        last_name = user_folder.split(\" \")[1]\n",
    "                        user_id = user_folder.split(\" \")[2]\n",
    "                \n",
    "                        migrate_videos_to_google_drive(file_path, first_name, last_name, user_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zoom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
