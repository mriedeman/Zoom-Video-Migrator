{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zoom to Google Drive\n",
    "\n",
    "The goal of this project is to transfer the entirety of Kinetic Potential's Zoom cloud recorded meetings to Google Drive."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do:\n",
    "\n",
    "- Test the memory buffer method in a loop.\n",
    "- env.example file and README for pass-off\n",
    "\n",
    "\n",
    "Completed:\n",
    "- Test the upload to google drive.\n",
    "- Test the upload to google drive using memory buffer.\n",
    "- Iterate through all files for all users to apply download function.\n",
    "- Test the download of all recordings in a single file.\n",
    "- Test the download of a single download url.\n",
    "- Retrieve download urls for every month intervel to present date.\n",
    "- Ensure every result is captured through next_page_token\n",
    "- Connect to Zoom Api using JWT to collect a month of download_urls\n",
    "- Proof-of-concept for download and transfer to google drive.\n",
    "- Get Admin Access for kpauditorium1\n",
    "- Set up JWT for ZOOM api/access keys\n",
    "- Setup Google Drive api access (Google Drive Parent Folder Id & JSON SERVICE_ACCOUNT_FILE)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof-of-Concept: Downloading cloud recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoomus import ZoomClient\n",
    "import os\n",
    "import wget\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API credentials from environment variables\n",
    "api_key = os.environ.get('ZOOM_API_KEY')\n",
    "api_secret = os.environ.get('ZOOM_API_SECRET')\n",
    "\n",
    "# Instantiate Zoom client\n",
    "zoom = ZoomClient(api_key=api_key, api_secret=api_secret)\n",
    "\n",
    "# Get list of completed recordings\n",
    "response = zoom.recording.list(user_id='me', status='completed')\n",
    "\n",
    "# Parse JSON response\n",
    "user_recordings = response.json()\n",
    "\n",
    "\n",
    "# reference the zoom api json keys => https://developers.zoom.us/docs/api/rest/reference/zoom-api/methods/#operation/recordingsList\n",
    "#just download the first meetings recorded files.\n",
    "for meeting in user_recordings['meetings']:\n",
    "#for meeting in user_recordings['meetings']:\n",
    "    for recording in meeting['recording_files']:\n",
    "        if recording['file_type'] == 'MP4':\n",
    "            url = recording['download_url']\n",
    "            filename = recording['id'] + '.mp4'\n",
    "            foldername = 'Zoom Recordings'\n",
    "            if not os.path.exists(foldername):\n",
    "                os.mkdir(foldername)\n",
    "            filepath = os.path.join(os.getcwd(), foldername, filename)\n",
    "            wget.download(url, filepath)\n",
    "            print(f\"{filename} downloaded successfully.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof-of-concept: Use Memory Buffer to Pass Recordings Retrieved from Zoom API to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from zoomus import ZoomClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get Zoom API credentials from environment variables\n",
    "api_key = os.environ.get('ZOOM_API_KEY')\n",
    "api_secret = os.environ.get('ZOOM_API_SECRET')\n",
    "print(api_key)\n",
    "print(api_secret)\n",
    "#Google Drive API credentials\n",
    "#changl\n",
    "os.environ['SERVICE_ACCOUNT_FILE'] = r'C:\\Users\\Michael\\Documents\\dev\\Secrets\\moonlit-parsec-382617-ba2e0dbfed17.json'\n",
    "if 'SERVICE_ACCOUNT_FILE' not in os.environ:\n",
    "    raise ValueError('SERVICE_ACCOUNT_FILE environment variable not set')\n",
    "google_api_credentials = os.environ.get('SERVICE_ACCOUNT_FILE')\n",
    "creds = service_account.Credentials.from_service_account_file(google_api_credentials)\n",
    "\n",
    "#The ID of the Folder where the videos will be stored\n",
    "#My personal zoom folder\n",
    "PARENT_FOLDER_ID = '15thhKA3nn9wRJvjQe0cQe271bm24PAnU'\n",
    "                    \n",
    "#KP Parent ID\n",
    "#1O6yxnkUv1-yqDt3DMxZphkwtPktEfnl5\n",
    "#Google Drive API client\n",
    "drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "#Instantiate Zoom client\n",
    "zoom = ZoomClient(api_key=api_key, api_secret=api_secret)\n",
    "\n",
    "#Get list of completed recordings\n",
    "response = zoom.recording.list(user_id='me', status='completed', from_='2021-01-01', to_='2023-04-10')\n",
    "\n",
    "#Parse JSON response\n",
    "user_recordings = response.json()\n",
    "\n",
    "#Iterate through Zoom API's meetings\n",
    "#remove [0] to do all meetings\n",
    "for meeting in user_recordings['meetings']:\n",
    "    #There can be multiple recordings in a single meeting, but typically one\n",
    "    for recording in meeting['recording_files']:\n",
    "        if recording['file_type'] == 'MP4':\n",
    "            video_id = recording['id']\n",
    "            filename = video_id + '.mp4'\n",
    "            url = recording['download_url']\n",
    "            \n",
    "            #Download video file to memory buffer\n",
    "            response = zoom.recording.get(recording_id=video_id, download=True)\n",
    "            video_bytes = io.BytesIO(response.content)\n",
    "            \n",
    "            #Upload video file to Google Drive\n",
    "            file_metadata = {'name': filename, 'parents': [PARENT_FOLDER_ID]}\n",
    "            media = io.BytesIO(video_bytes.getvalue())\n",
    "            drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "            print(f\"{filename} uploaded successfully to Google Drive.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof-of-Concept: Access Download URLs for a single month intervel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import jwt\n",
    "import datetime\n",
    "#Get new jwt Token\n",
    "api_key = 'xxxxxxx'\n",
    "api_secret = 'xxxxxxx'\n",
    "\n",
    "payload = {\n",
    "    'iss': api_key,\n",
    "    'exp': datetime.datetime.utcnow() + datetime.timedelta(minutes=10)\n",
    "}\n",
    "\n",
    "token = jwt.encode(payload, api_secret)\n",
    "\n",
    "#get user ids to iterate through\n",
    "with open('users.json') as f:\n",
    "    users = json.load(f)\n",
    "\n",
    "# base url\n",
    "base_url = \"https://api.zoom.us/v2\"\n",
    "\n",
    "headers = {'Authorization': f'Bearer {token}'}\n",
    "\n",
    "recordings_data = []\n",
    "for user in users['users']:\n",
    "    user_id = user[\"id\"]\n",
    "    \n",
    "    url = base_url + f\"/users/{user_id}/recordings\"\n",
    "    params = { \"from\": \"2021-01-01\",\n",
    "                \"to\": \"2021-02-01\"}\n",
    "    \n",
    "    response = requests.get(url, headers=headers, params = params)\n",
    "\n",
    "    recordings = response.json()\n",
    "    recordings_data.append(recordings)\n",
    "\n",
    "    print(recordings['meetings'])\n",
    "\n",
    "with open('recordings.json', 'w') as f:\n",
    "    json.dump(recordings_data, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Function: Generate JWT Token From API Key & Secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJtWVhFZjVPZVQwaWF0RHI1M3c3RzlBIiwiZXhwIjoxNjgyMzcxMjQzfQ.c_362JisAMnFTbH0Cvbc36BiP-lgpbHuLxADallfuPQ'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import jwt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_jwt_token():\n",
    "# Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    # Get API credentials from environment variables\n",
    "    api_key = os.environ.get('ZOOM_API_KEY')\n",
    "    if not api_key:\n",
    "        raise ValueError('ZOOM_API_KEY environment variable not set')\n",
    "\n",
    "    api_secret = os.environ.get('ZOOM_API_SECRET')\n",
    "    if not api_secret:\n",
    "        raise ValueError('ZOOM_API_KEY environment variable not set')\n",
    "\n",
    "    payload = {\n",
    "        'iss': api_key,\n",
    "        'exp': datetime.utcnow() + timedelta(minutes=10)\n",
    "    }\n",
    "\n",
    "    token = jwt.encode(payload, api_secret)\n",
    "    return token\n",
    "\n",
    "generate_jwt_token()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Function: Update the \"users.json\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated the api_data/users/users.json to include any new users on 2023-04-17 11:53:40.859592\n"
     ]
    }
   ],
   "source": [
    "#Collect updated list of user's\n",
    "from zoomus import ZoomClient\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def update_users():\n",
    "    api_key = os.environ.get('ZOOM_API_KEY')\n",
    "    if not api_key:\n",
    "        raise ValueError('ZOOM_API_KEY environment variable not set')\n",
    "\n",
    "    api_secret = os.environ.get('ZOOM_API_SECRET')\n",
    "    if not api_secret:\n",
    "        raise ValueError('ZOOM_API_SECRET environment variable not set')\n",
    "\n",
    "    zoom = ZoomClient(api_key=api_key, api_secret=api_secret)\n",
    "\n",
    "    users_response = zoom.user.list().json()\n",
    "\n",
    "    filename = 'api_data/users/users.json'\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(users_response, f)\n",
    "\n",
    "    print(f\"Updated the {filename} to include any new users on {datetime.now()}\")\n",
    "\n",
    "update_users()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Function: Generate a list of \"From\" and \"To\" Date Parameters\n",
    "\n",
    "The purpose of this function is to collect date ranges to collect legacy videos for the initial video transfer. After the initial transfer a new function will be developed for a weekly video transfer protocol. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2021-01-01', '2021-01-31'),\n",
       " ('2021-02-01', '2021-02-28'),\n",
       " ('2021-03-01', '2021-03-31'),\n",
       " ('2021-04-01', '2021-04-30'),\n",
       " ('2021-05-01', '2021-05-31'),\n",
       " ('2021-06-01', '2021-06-30'),\n",
       " ('2021-07-01', '2021-07-31'),\n",
       " ('2021-08-01', '2021-08-31'),\n",
       " ('2021-09-01', '2021-09-30'),\n",
       " ('2021-10-01', '2021-10-31'),\n",
       " ('2021-11-01', '2021-11-30'),\n",
       " ('2021-12-01', '2021-12-31'),\n",
       " ('2022-01-01', '2022-01-31'),\n",
       " ('2022-02-01', '2022-02-28'),\n",
       " ('2022-03-01', '2022-03-31'),\n",
       " ('2022-04-01', '2022-04-30'),\n",
       " ('2022-05-01', '2022-05-31'),\n",
       " ('2022-06-01', '2022-06-30'),\n",
       " ('2022-07-01', '2022-07-31'),\n",
       " ('2022-08-01', '2022-08-31'),\n",
       " ('2022-09-01', '2022-09-30'),\n",
       " ('2022-10-01', '2022-10-31'),\n",
       " ('2022-11-01', '2022-11-30'),\n",
       " ('2022-12-01', '2022-12-31'),\n",
       " ('2023-01-01', '2023-01-31'),\n",
       " ('2023-02-01', '2023-02-28'),\n",
       " ('2023-03-01', '2023-03-31'),\n",
       " ('2023-04-01', '2023-04-30')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def date_params():\n",
    "    start_date_str = \"2021-01-01\" #first zoom recording was 2021-01-03\n",
    "    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "    now = datetime.now()\n",
    "\n",
    "    date_ranges = []\n",
    "    while start_date < now:\n",
    "        # Get the first day of the current month\n",
    "        start_of_month = datetime(start_date.year, start_date.month, 1)\n",
    "\n",
    "        #Get the last day of the current month\n",
    "        end_of_month = (start_of_month + timedelta(days=32)).replace(day=1) - timedelta(days=1)\n",
    "\n",
    "        date_ranges.append((start_of_month.strftime('%Y-%m-%d'), end_of_month.strftime('%Y-%m-%d')))\n",
    "\n",
    "        start_date = end_of_month + timedelta(days=1)\n",
    "\n",
    "    #print(\"Created a tupled list of FROM and TO parameters for the Zoom API Query\")\n",
    "    return date_ranges\n",
    "\n",
    "date_params()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect All recording data for a single user - No need to run anymore that loop is working. \n",
    "\n",
    "* Some issues with pagination here, the loops seems to run continuously and I don't want to overwhelm zoom with requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import jwt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#get user ids to iterate through\n",
    "with open('./api_data/users/users.json') as f:\n",
    "    users = json.load(f)\n",
    "\n",
    "\n",
    "#testing single user now, will turn to for loop\n",
    "user = users['users'][0]\n",
    "user_first_name = user['first_name']\n",
    "user_last_name = user['last_name']\n",
    "user_id = user['id']\n",
    "\n",
    "for start, end in date_params():\n",
    "\n",
    "    #Avoid sending API query with end date greater than today's date\n",
    "    now = datetime.now()\n",
    "    if datetime.strptime(end, '%Y-%m-%d') > now:\n",
    "        break\n",
    "    \n",
    "    # API access\n",
    "    base_url = \"https://api.zoom.us/v2\"\n",
    "    token = generate_jwt_token()\n",
    "    headers = {'Authorization': f'Bearer {token}'}\n",
    "    url = base_url + f\"/users/{user_id}/recordings\"\n",
    "    params = { \"from\": start,\n",
    "               \"to\": end}\n",
    "    \n",
    "    response = requests.get(url, headers=headers, params = params)\n",
    "    recordings_data = response.json()\n",
    "    \n",
    "    # Keep making API requests until all pages are collected\n",
    "    next_token = recordings_data.get('next_page_token')\n",
    "    while next_token:\n",
    "        \n",
    "        url = base_url + f\"/users/{user_id}/recordings\"\n",
    "        params = { \"from\": start, \"to\": end, \"next_page_token\": next_token}\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        next_page_data = response.json()\n",
    "        recordings_data['meetings'] += next_page_data['meetings']\n",
    "        next_token = next_page_data.get('next_page_token')\n",
    "        if next_token == recordings_data.get('next_page_token'):\n",
    "            break\n",
    "        recordings_data['next_page_token'] = next_token\n",
    "        \n",
    "    print(f'Collecting recording results for user {user_first_name} {user_last_name} from {start} to {end}')\n",
    "\n",
    "    #Store data\n",
    "    directory = f'api_data/recordings/{user_first_name} {user_last_name}{user_id}'\n",
    "    filename = f'api_data/recordings/{user_first_name} {user_last_name}{user_id}/{start} to {end}.json'\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(recordings_data, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Function: Collect All Recording Data for All Users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting recording results for user Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\n",
      "Collecting recording results for user Jim Smith 2UsBai07StauaBqBbbjxeA\n",
      "Collecting recording results for user Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\n",
      "Collecting recording results for user STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\n",
      "Collecting recording results for user Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\n",
      "Collecting recording results for user KP Auditorium f98iwlEeRpS2XAipG1j2YA\n",
      "Collecting recording results for user Dipayan Roy k3gb5SCTRDO00ko71PWNqw\n",
      "Collecting recording results for user Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\n",
      "Collecting recording results for user KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\n",
      "Collecting recording results for user Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\n",
      "DATA COLLECTION COMPLETE AT 2023-04-17 11:58:36.136408\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import jwt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def collect_recording_data():\n",
    "    #get user ids for API call \"/users/{user_id}/recordings\"\n",
    "    with open('./api_data/users/users.json') as f:\n",
    "        users = json.load(f)\n",
    "\n",
    "\n",
    "    #Collect all recording data, one user at a time\n",
    "    for user in users['users']:\n",
    "\n",
    "        user_first_name = user['first_name']\n",
    "        user_last_name = user['last_name']\n",
    "        user_id = user['id']\n",
    "        print(f'Collecting recording results for user {user_first_name} {user_last_name} {user_id}')\n",
    "        \n",
    "        for start, end in date_params():\n",
    "\n",
    "            #Avoid sending API query with end date greater than today's date\n",
    "            now = datetime.now()\n",
    "            if datetime.strptime(end, '%Y-%m-%d') > now:\n",
    "                break\n",
    "            \n",
    "            # API access\n",
    "            base_url = \"https://api.zoom.us/v2\"\n",
    "            token = generate_jwt_token()\n",
    "            headers = {'Authorization': f'Bearer {token}'}\n",
    "            url = base_url + f\"/users/{user_id}/recordings\"\n",
    "            params = { \"from\": start,\n",
    "                    \"to\": end}\n",
    "            \n",
    "            response = requests.get(url, headers=headers, params = params)\n",
    "            recordings_data = response.json()\n",
    "            \n",
    "            # Keep making API requests until all pages are collected\n",
    "            next_token = recordings_data.get('next_page_token')\n",
    "            while next_token:\n",
    "                \n",
    "                url = base_url + f\"/users/{user_id}/recordings\"\n",
    "                params = { \"from\": start, \"to\": end, \"next_page_token\": next_token}\n",
    "                response = requests.get(url, headers=headers, params=params)\n",
    "                next_page_data = response.json()\n",
    "                recordings_data['meetings'] += next_page_data['meetings']\n",
    "                next_token = next_page_data.get('next_page_token')\n",
    "                if next_token == recordings_data.get('next_page_token'):\n",
    "                    break\n",
    "                recordings_data['next_page_token'] = next_token\n",
    "                \n",
    "            #print(f'Collecting recording results for user {user_first_name} {user_last_name} from {start} to {end}')\n",
    "\n",
    "            #Store data\n",
    "            directory = f'api_data/recordings/{user_first_name} {user_last_name} {user_id}'\n",
    "            filename = f'api_data/recordings/{user_first_name} {user_last_name} {user_id}/{start} to {end}.json'\n",
    "\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            \n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(recordings_data, f)\n",
    "\n",
    "    print(f\"DATA COLLECTION COMPLETE AT {datetime.now()}\")\n",
    "\n",
    "collect_recording_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Function: Download a single video using the download_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import jwt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def download_video(download_url, first_name, last_name, user_id, filename):\n",
    "\n",
    "    token = generate_jwt_token()\n",
    "    response = requests.get(download_url + \"?access_token=\" + token, stream=True)\n",
    "\n",
    "    with open(f\"./videos/{first_name}_{last_name}_{user_id}/{filename}.mp4\", \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Function: Download all videos in a single file.\n",
    "\n",
    "Download_Urls Only Last 24 hours, rerun the ***collect_recording_data*** function to update the download urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import jwt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#single file\n",
    "def download_videos_in_file(file_path, first_name, last_name, user_id):\n",
    "\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    #Download Every MP4 Recording for ALl Meetings In the File recording['file_type'] == \"MP4\"\n",
    "    for meeting in data['meetings']:\n",
    "        folder = f\"{first_name}_{last_name}_{user_id}\"\n",
    "        for recording in meeting['recording_files']:\n",
    "            if recording['file_type'] == \"MP4\":\n",
    "\n",
    "                download_url = recording['download_url']\n",
    "                date_string = recording['recording_start']\n",
    "                dt_object = datetime.strptime(date_string, '%Y-%m-%dT%H:%M:%SZ')\n",
    "                filename = dt_object.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "                video_directory = f\"./videos/{folder}\"\n",
    "                \n",
    "                if not os.path.exists(video_directory):\n",
    "                    os.makedirs(video_directory)\n",
    "\n",
    "                download_video(download_url, first_name, last_name, user_id, filename)\n",
    "\n",
    "#EXAMPLE:\n",
    "#download_videos_in_file(\"Dipayan\", \"Roy\", \"k3gb5SCTRDO00ko71PWNqw\", \"./api_data/recordings/Dipayan Royk3gb5SCTRDO00ko71PWNqw/2021-01-01 to 2021-01-31.json\")\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloads every video for every file (will use memory buffer instead to upload to cloud)\n",
    "\n",
    "Purpose of this code is to iterate through the file directory to grab all download urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2021-01-01 to 2021-01-31.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2021-02-01 to 2021-02-28.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2021-03-01 to 2021-03-31.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2021-04-01 to 2021-04-30.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2021-05-01 to 2021-05-31.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2021-06-01 to 2021-06-30.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2021-07-01 to 2021-07-31.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2021-08-01 to 2021-08-31.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2021-09-01 to 2021-09-30.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2021-10-01 to 2021-10-31.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2021-11-01 to 2021-11-30.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2021-12-01 to 2021-12-31.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2022-01-01 to 2022-01-31.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2022-02-01 to 2022-02-28.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2022-03-01 to 2022-03-31.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2022-04-01 to 2022-04-30.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2022-05-01 to 2022-05-31.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2022-06-01 to 2022-06-30.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2022-07-01 to 2022-07-31.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2022-08-01 to 2022-08-31.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2022-09-01 to 2022-09-30.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2022-10-01 to 2022-10-31.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2022-11-01 to 2022-11-30.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2022-12-01 to 2022-12-31.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2023-01-01 to 2023-01-31.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2023-02-01 to 2023-02-28.json\n",
      "Dipayan  ---  Roy  ---  k3gb5SCTRDO00ko71PWNqw  ---  ./api_data/recordings\\Dipayan Roy k3gb5SCTRDO00ko71PWNqw\\2023-03-01 to 2023-03-31.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2021-01-01 to 2021-01-31.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2021-02-01 to 2021-02-28.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2021-03-01 to 2021-03-31.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2021-04-01 to 2021-04-30.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2021-05-01 to 2021-05-31.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2021-06-01 to 2021-06-30.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2021-07-01 to 2021-07-31.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2021-08-01 to 2021-08-31.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2021-09-01 to 2021-09-30.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2021-10-01 to 2021-10-31.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2021-11-01 to 2021-11-30.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2021-12-01 to 2021-12-31.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2022-01-01 to 2022-01-31.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2022-02-01 to 2022-02-28.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2022-03-01 to 2022-03-31.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2022-04-01 to 2022-04-30.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2022-05-01 to 2022-05-31.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2022-06-01 to 2022-06-30.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2022-07-01 to 2022-07-31.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2022-08-01 to 2022-08-31.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2022-09-01 to 2022-09-30.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2022-10-01 to 2022-10-31.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2022-11-01 to 2022-11-30.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2022-12-01 to 2022-12-31.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2023-01-01 to 2023-01-31.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2023-02-01 to 2023-02-28.json\n",
      "Jim  ---  Smith  ---  2UsBai07StauaBqBbbjxeA  ---  ./api_data/recordings\\Jim Smith 2UsBai07StauaBqBbbjxeA\\2023-03-01 to 2023-03-31.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2021-01-01 to 2021-01-31.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2021-02-01 to 2021-02-28.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2021-03-01 to 2021-03-31.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2021-04-01 to 2021-04-30.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2021-05-01 to 2021-05-31.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2021-06-01 to 2021-06-30.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2021-07-01 to 2021-07-31.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2021-08-01 to 2021-08-31.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2021-09-01 to 2021-09-30.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2021-10-01 to 2021-10-31.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2021-11-01 to 2021-11-30.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2021-12-01 to 2021-12-31.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2022-01-01 to 2022-01-31.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2022-02-01 to 2022-02-28.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2022-03-01 to 2022-03-31.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2022-04-01 to 2022-04-30.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2022-05-01 to 2022-05-31.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2022-06-01 to 2022-06-30.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2022-07-01 to 2022-07-31.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2022-08-01 to 2022-08-31.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2022-09-01 to 2022-09-30.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2022-10-01 to 2022-10-31.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2022-11-01 to 2022-11-30.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2022-12-01 to 2022-12-31.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2023-01-01 to 2023-01-31.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2023-02-01 to 2023-02-28.json\n",
      "Jim  ---  Smith  ---  5TTfg6YGQxG1xlI9h_NTxQ  ---  ./api_data/recordings\\Jim Smith 5TTfg6YGQxG1xlI9h_NTxQ\\2023-03-01 to 2023-03-31.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2021-01-01 to 2021-01-31.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2021-02-01 to 2021-02-28.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2021-03-01 to 2021-03-31.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2021-04-01 to 2021-04-30.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2021-05-01 to 2021-05-31.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2021-06-01 to 2021-06-30.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2021-07-01 to 2021-07-31.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2021-08-01 to 2021-08-31.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2021-09-01 to 2021-09-30.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2021-10-01 to 2021-10-31.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2021-11-01 to 2021-11-30.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2021-12-01 to 2021-12-31.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2022-01-01 to 2022-01-31.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2022-02-01 to 2022-02-28.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2022-03-01 to 2022-03-31.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2022-04-01 to 2022-04-30.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2022-05-01 to 2022-05-31.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2022-06-01 to 2022-06-30.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2022-07-01 to 2022-07-31.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2022-08-01 to 2022-08-31.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2022-09-01 to 2022-09-30.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2022-10-01 to 2022-10-31.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2022-11-01 to 2022-11-30.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2022-12-01 to 2022-12-31.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2023-01-01 to 2023-01-31.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2023-02-01 to 2023-02-28.json\n",
      "Kinetic  ---  Potential  ---  0Kv63NSFTpa7uH6Vh_gbiQ  ---  ./api_data/recordings\\Kinetic Potential 0Kv63NSFTpa7uH6Vh_gbiQ\\2023-03-01 to 2023-03-31.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2021-01-01 to 2021-01-31.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2021-02-01 to 2021-02-28.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2021-03-01 to 2021-03-31.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2021-04-01 to 2021-04-30.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2021-05-01 to 2021-05-31.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2021-06-01 to 2021-06-30.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2021-07-01 to 2021-07-31.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2021-08-01 to 2021-08-31.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2021-09-01 to 2021-09-30.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2021-10-01 to 2021-10-31.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2021-11-01 to 2021-11-30.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2021-12-01 to 2021-12-31.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2022-01-01 to 2022-01-31.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2022-02-01 to 2022-02-28.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2022-03-01 to 2022-03-31.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2022-04-01 to 2022-04-30.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2022-05-01 to 2022-05-31.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2022-06-01 to 2022-06-30.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2022-07-01 to 2022-07-31.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2022-08-01 to 2022-08-31.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2022-09-01 to 2022-09-30.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2022-10-01 to 2022-10-31.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2022-11-01 to 2022-11-30.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2022-12-01 to 2022-12-31.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2023-01-01 to 2023-01-31.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2023-02-01 to 2023-02-28.json\n",
      "Kinetic  ---  Potential  ---  bD_8YzDlQoe2gUQCMmMrHQ  ---  ./api_data/recordings\\Kinetic Potential bD_8YzDlQoe2gUQCMmMrHQ\\2023-03-01 to 2023-03-31.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2021-01-01 to 2021-01-31.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2021-02-01 to 2021-02-28.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2021-03-01 to 2021-03-31.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2021-04-01 to 2021-04-30.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2021-05-01 to 2021-05-31.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2021-06-01 to 2021-06-30.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2021-07-01 to 2021-07-31.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2021-08-01 to 2021-08-31.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2021-09-01 to 2021-09-30.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2021-10-01 to 2021-10-31.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2021-11-01 to 2021-11-30.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2021-12-01 to 2021-12-31.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2022-01-01 to 2022-01-31.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2022-02-01 to 2022-02-28.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2022-03-01 to 2022-03-31.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2022-04-01 to 2022-04-30.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2022-05-01 to 2022-05-31.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2022-06-01 to 2022-06-30.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2022-07-01 to 2022-07-31.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2022-08-01 to 2022-08-31.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2022-09-01 to 2022-09-30.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2022-10-01 to 2022-10-31.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2022-11-01 to 2022-11-30.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2022-12-01 to 2022-12-31.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2023-01-01 to 2023-01-31.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2023-02-01 to 2023-02-28.json\n",
      "Kinetic  ---  Potential  ---  kcyBlEKSScWcRI_T7wTCyA  ---  ./api_data/recordings\\Kinetic Potential kcyBlEKSScWcRI_T7wTCyA\\2023-03-01 to 2023-03-31.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2021-01-01 to 2021-01-31.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2021-02-01 to 2021-02-28.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2021-03-01 to 2021-03-31.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2021-04-01 to 2021-04-30.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2021-05-01 to 2021-05-31.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2021-06-01 to 2021-06-30.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2021-07-01 to 2021-07-31.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2021-08-01 to 2021-08-31.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2021-09-01 to 2021-09-30.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2021-10-01 to 2021-10-31.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2021-11-01 to 2021-11-30.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2021-12-01 to 2021-12-31.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2022-01-01 to 2022-01-31.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2022-02-01 to 2022-02-28.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2022-03-01 to 2022-03-31.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2022-04-01 to 2022-04-30.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2022-05-01 to 2022-05-31.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2022-06-01 to 2022-06-30.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2022-07-01 to 2022-07-31.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2022-08-01 to 2022-08-31.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2022-09-01 to 2022-09-30.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2022-10-01 to 2022-10-31.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2022-11-01 to 2022-11-30.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2022-12-01 to 2022-12-31.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2023-01-01 to 2023-01-31.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2023-02-01 to 2023-02-28.json\n",
      "Kinetic  ---  Potential  ---  VHAAMlYxTuiU4PUz41t41Q  ---  ./api_data/recordings\\Kinetic Potential VHAAMlYxTuiU4PUz41t41Q\\2023-03-01 to 2023-03-31.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2021-01-01 to 2021-01-31.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2021-02-01 to 2021-02-28.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2021-03-01 to 2021-03-31.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2021-04-01 to 2021-04-30.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2021-05-01 to 2021-05-31.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2021-06-01 to 2021-06-30.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2021-07-01 to 2021-07-31.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2021-08-01 to 2021-08-31.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2021-09-01 to 2021-09-30.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2021-10-01 to 2021-10-31.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2021-11-01 to 2021-11-30.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2021-12-01 to 2021-12-31.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2022-01-01 to 2022-01-31.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2022-02-01 to 2022-02-28.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2022-03-01 to 2022-03-31.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2022-04-01 to 2022-04-30.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2022-05-01 to 2022-05-31.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2022-06-01 to 2022-06-30.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2022-07-01 to 2022-07-31.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2022-08-01 to 2022-08-31.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2022-09-01 to 2022-09-30.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2022-10-01 to 2022-10-31.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2022-11-01 to 2022-11-30.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2022-12-01 to 2022-12-31.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2023-01-01 to 2023-01-31.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2023-02-01 to 2023-02-28.json\n",
      "KP  ---  Conference  ---  Room  ---  ./api_data/recordings\\KP Conference Room SwUXkvbdQpC2d4oy_N5F0A\\2023-03-01 to 2023-03-31.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2021-01-01 to 2021-01-31.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2021-02-01 to 2021-02-28.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2021-03-01 to 2021-03-31.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2021-04-01 to 2021-04-30.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2021-05-01 to 2021-05-31.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2021-06-01 to 2021-06-30.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2021-07-01 to 2021-07-31.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2021-08-01 to 2021-08-31.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2021-09-01 to 2021-09-30.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2021-10-01 to 2021-10-31.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2021-11-01 to 2021-11-30.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2021-12-01 to 2021-12-31.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2022-01-01 to 2022-01-31.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2022-02-01 to 2022-02-28.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2022-03-01 to 2022-03-31.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2022-04-01 to 2022-04-30.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2022-05-01 to 2022-05-31.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2022-06-01 to 2022-06-30.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2022-07-01 to 2022-07-31.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2022-08-01 to 2022-08-31.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2022-09-01 to 2022-09-30.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2022-10-01 to 2022-10-31.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2022-11-01 to 2022-11-30.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2022-12-01 to 2022-12-31.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2023-01-01 to 2023-01-31.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2023-02-01 to 2023-02-28.json\n",
      "STAR  ---  Navigators  ---  Ack9LeaxSYu_oOtiMllRIQ  ---  ./api_data/recordings\\STAR Navigators Ack9LeaxSYu_oOtiMllRIQ\\2023-03-01 to 2023-03-31.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "recording_directory = \"./api_data/recordings\"\n",
    "for user_folder in os.listdir(recording_directory):\n",
    "        path = os.path.isdir(os.path.join(recording_directory, user_folder))\n",
    "        if path:\n",
    "            for file_name in os.listdir(os.path.join(recording_directory, user_folder)):\n",
    "                  if file_name.endswith(\".json\"):\n",
    "                        #print(os.path.join(recording_directory, user_folder, file_name)) #270 json file paths\n",
    "\n",
    "                        #implement download videos function\n",
    "                        #pass it the first_name, last_name, user_id, and file path\n",
    "                        #parse the folder name to extract first_name, last_name, and user_id\n",
    "                        file_path = os.path.join(recording_directory, user_folder, file_name)\n",
    "                        first_name = user_folder.split(\" \")[0]\n",
    "                        last_name = user_folder.split(\" \")[1]\n",
    "                        user_id = user_folder.split(\" \")[2]\n",
    "\n",
    "                        print(first_name, \" --- \", last_name, \" --- \", user_id , \" --- \", file_path)\n",
    "                        #UNCOMMENT THIS FUNCTION IF YOU WOULD LIKE TO DOWNLOAD ALL OFF THE VIDEOS FOR ALL USERS\n",
    "                        #download_videos_in_file(file_path, first_name, last_name, user_id)\n",
    "                        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Drive"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Google Drive Connection: Upload a simple text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15thhKA3nn9wRJvjQe0cQe271bm24PAnU\n",
      "Uploaded File with a File ID of: 1e0awKMHIynCTWuQVzh7eDcvIlSRyhbrA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Google Drive API credentials\n",
    "if 'GOOGLE_AUTH_FILE' not in os.environ:\n",
    "    raise ValueError('GOOGLE_AUTH_FILE environment variable not set')\n",
    "google_api_credentials = os.environ.get('GOOGLE_AUTH_FILE')\n",
    "creds = service_account.Credentials.from_service_account_file(google_api_credentials)\n",
    "\n",
    "if 'GOOGLE_PARENT_FOLDER_ID' not in os.environ:\n",
    "    raise ValueError('GOOGLE_PARENT_FOLDER_ID environment variable not set')\n",
    "parent_folder_id = os.environ.get('GOOGLE_PARENT_FOLDER_ID')\n",
    "print(parent_folder_id)\n",
    "# Create Google Drive API client\n",
    "drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# File to be uploaded\n",
    "file_name = \"michael.txt\"\n",
    "file_path = r\"C:\\Users\\Michael\\Documents\\dev\\KP\\KineticPotential\\DataEngineering\\zoom\\michael.txt\"\n",
    "# Upload file to Google Drive\n",
    "file_metadata = {'name': file_name, 'parents': [parent_folder_id]}\n",
    "media = MediaFileUpload(file_path, resumable=True)\n",
    "\n",
    "file = drive_service.files().create(\n",
    "    body=file_metadata,\n",
    "    media_body=media,\n",
    "    fields='id'\n",
    ").execute()\n",
    "\n",
    "print(f\"Uploaded File with a File ID of: {file.get('id')}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Single Video File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-04_15-57-22.mp4 uploaded successfully to Google Drive.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaIoBaseUpload\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "    #specs for a single video\n",
    "file_path = r\"C:\\Users\\Michael\\Documents\\dev\\KP\\KineticPotential\\DataEngineering\\zoom\\videos\\Dipayan_Roy_k3gb5SCTRDO00ko71PWNqw\\2021-01-04_14-55-31.mp4\"\n",
    "filename = '2021-01-04_15-57-22.mp4'\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "#Google Drive API credentials\n",
    "if 'GOOGLE_AUTH_FILE' not in os.environ:\n",
    "    raise ValueError('GOOGLE_AUTH_FILE environment variable not set')\n",
    "google_api_credentials = os.environ.get('GOOGLE_AUTH_FILE')\n",
    "creds = service_account.Credentials.from_service_account_file(google_api_credentials)\n",
    "\n",
    "if 'GOOGLE_PARENT_FOLDER_ID' not in os.environ:\n",
    "    raise ValueError('GOOGLE_PARENT_FOLDER_ID environment variable not set')\n",
    "parent_folder_id= os.environ.get('GOOGLE_PARENT_FOLDER_ID')\n",
    "\n",
    "\n",
    "\n",
    "drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "\n",
    "\n",
    "# Read the file into a BytesIO object to simulate downloading through a memory buffer\n",
    "with open(file_path, 'rb') as f:\n",
    "    file_data = f.read()\n",
    "    media = MediaIoBaseUpload(io.BytesIO(file_data), mimetype='video/mp4', chunksize=1024*1024, resumable=True)\n",
    "\n",
    "\n",
    "#Upload video file to Google Drive\n",
    "file_metadata = {'name': filename, 'parents': [parent_folder_id]}\n",
    "#media = io.BytesIO(video_bytes.getvalue())\n",
    "\n",
    "drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "\n",
    "print(f\"{filename} uploaded successfully to Google Drive with a File ID of: {file.get('id')}.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Migration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaIoBaseUpload\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def download_video_to_buffer(download_url):\n",
    "    token = generate_jwt_token()\n",
    "    response = requests.get(download_url + \"?access_token=\" + token, stream=True)\n",
    "    video_bytes = io.BytesIO()\n",
    "    for chunk in response.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            video_bytes.write(chunk)\n",
    "    video_bytes.seek(0)\n",
    "    return video_bytes\n",
    "\n",
    "\n",
    "def upload_buffer_to_drive(video_bytes, filename):\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    # Google Drive API credentials\n",
    "    if 'GOOGLE_AUTH_FILE' not in os.environ:\n",
    "        raise ValueError('GOOGLE_AUTH_FILE environment variable not set')\n",
    "    google_api_credentials = os.environ.get('GOOGLE_AUTH_FILE')\n",
    "    creds = service_account.Credentials.from_service_account_file(google_api_credentials)\n",
    "\n",
    "    if 'GOOGLE_PARENT_FOLDER_ID' not in os.environ:\n",
    "        raise ValueError('GOOGLE_PARENT_FOLDER_ID environment variable not set')\n",
    "    parent_folder_id = os.environ.get('GOOGLE_PARENT_FOLDER_ID')\n",
    "\n",
    "    drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    # Create a MediaIoBaseUpload object using the video_bytes\n",
    "    media = MediaIoBaseUpload(video_bytes, mimetype='video/mp4', chunksize=1024 * 1024, resumable=True)\n",
    "\n",
    "    # Upload video file to Google Drive\n",
    "    file_metadata = {'name': filename, 'parents': [parent_folder_id]}\n",
    "    file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "\n",
    "    print(f\"{filename} uploaded successfully to Google Drive with a File ID of: {file.get('id')}.\")\n",
    "\n",
    "\n",
    "def migrate_videos(json_file, first_name, last_name):\n",
    "    \"\"\"Migrates All The Videos in a Single JSON File to Google Drive\"\"\"\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for meeting in data['meetings']:\n",
    "        for recording in meeting['recording_files']:\n",
    "            if recording['file_type'] == \"MP4\":\n",
    "\n",
    "                download_url = recording['download_url']\n",
    "                date_string = recording['recording_start']\n",
    "                dt_object = datetime.strptime(date_string, '%Y-%m-%dT%H:%M:%SZ')\n",
    "                timestamp_string = dt_object.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "                filename = f\"{first_name} {last_name} {timestamp_string}\"\n",
    "                video = download_video_to_buffer(download_url)\n",
    "                upload_buffer_to_drive(video, filename)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migrate All Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import jwt\n",
    "from datetime import datetime, timedelta\n",
    "from zoomus import ZoomClient\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaIoBaseUpload\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "recording_directory = \"./api_data/recordings\"\n",
    "for user_folder in os.listdir(recording_directory):\n",
    "        path = os.path.isdir(os.path.join(recording_directory, user_folder))\n",
    "        if path:\n",
    "            for file_name in os.listdir(os.path.join(recording_directory, user_folder)):\n",
    "                  if file_name.endswith(\".json\"):\n",
    "                        \n",
    "                        file_path = os.path.join(recording_directory, user_folder, file_name)\n",
    "                        first_name = user_folder.split(\" \")[0]\n",
    "                        last_name = user_folder.split(\" \")[1]\n",
    "                        user_id = user_folder.split(\" \")[2]\n",
    "                        migrate_videos(file_path, first_name, last_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zoom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
